{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b86f3a27-d7fa-49e4-862d-fee03934713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import impute\n",
    "from sklearn import feature_selection\n",
    "from sklearn import tree\n",
    "from copy import deepcopy\n",
    "\n",
    "def loadFile(filePath):\n",
    "    dataset = []\n",
    "    with open(filePath, 'r') as fd:\n",
    "        header = next(fd)\n",
    "        reader = csv.reader(fd, delimiter=';')\n",
    "        for row in reader:\n",
    "            for i in range(len(row)):\n",
    "                row[i] = float(row[i])\n",
    "            dataset.append(row)\n",
    "    return dataset, header.replace('\"', '').split(';')\n",
    "\n",
    "def removeValues(dataset, percentage):\n",
    "    copy = deepcopy(dataset)\n",
    "    toRemove = int(len(dataset[0])*percentage)\n",
    "    print(len(dataset), percentage, toRemove)\n",
    "    for row in copy:\n",
    "        for i in range(toRemove):\n",
    "            feature = int(random.random() * len(row)) \n",
    "            row[feature] = np.nan\n",
    "    return copy\n",
    "\n",
    "def handleMissingValues(dataset, strat):\n",
    "    imp = sk.impute.SimpleImputer(missing_values=np.nan, strategy=strat)\n",
    "    return imp.fit_transform(dataset)\n",
    "\n",
    "def applyNormalization(dataset, norm):\n",
    "    return sk.preprocessing.normalize(dataset, norm=norm)\n",
    "\n",
    "def applyDiscretization(dataset):\n",
    "    transf = sk.preprocessing.KBinsDiscretizer(n_bins = 3, encode = 'ordinal', strategy = 'quantile')\n",
    "    return transf.fit_transform(dataset)\n",
    "\n",
    "def applyDataReduction(dataset):\n",
    "    agglo = sk.cluster.FeatureAgglomeration(n_clusters=5)\n",
    "    return agglo.fit_transform(dataset)\n",
    "\n",
    "def pearsonCorrelation(dataset, header, target):\n",
    "    dictPearson = {}\n",
    "    df = pd.DataFrame(dataset, columns = header)\n",
    "    y = df[target]          #Target Variable\n",
    "    X = df.drop(target, axis=1)   #Feature Matrix\n",
    "    pearson = sk.feature_selection.r_regression(X, y, center=True)\n",
    "    return pearson\n",
    "\n",
    "def plotPearson(dataset, header, target, colors):\n",
    "    pearson = pearsonCorrelation(dataset, header, target)\n",
    "    headerPearson = deepcopy(header)\n",
    "    headerPearson.remove(target)\n",
    "    plt.title('Target Feature: ' + target)\n",
    "    plt.barh([1,2,3,4,5,6,7,8,9,10,11], pearson, height=0.8, align='center', tick_label=headerPearson,\n",
    "             color=colors)\n",
    "    plt.xlim([-1,1])\n",
    "    plt.show()\n",
    "\n",
    "def plotAllPearson(dataset, header):\n",
    "    colors = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(11)]\n",
    "    for target in header:\n",
    "        plotPearson(dataset, header, target, colors)\n",
    "    \n",
    "def randomColors(n):\n",
    "    colors = []\n",
    "    for i in range(n):\n",
    "        colors.append(np.random.choice(range(256), size=3))\n",
    "    print(colors)\n",
    "    return colors\n",
    "\n",
    "def decisionTree(df, X, target):\n",
    "    y = df[target]          #Target Variable\n",
    "    clf = tree.DecisionTreeClassifier(splitter='best', max_features = 11, random_state = 1)\n",
    "    clf = clf.fit(X, y)\n",
    "    tree.export_graphviz(clf, out_file='tree.dot', feature_names=header[:-1], class_names=[\"bad\",\"good\",\"medium\"]) \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    tree.plot_tree(clf, feature_names=header, class_names=[\"bad\",\"good\",\"medium\"])\n",
    "    #ainda só está a criar a arvore, falta por a fazer a classificação\n",
    "    \n",
    "def ourMLPClassifier(df, X, target):\n",
    "    y = df[target]\n",
    "    X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(X, y, stratify=y, random_state=1)\n",
    "    clf = sk.neural_network.MLPClassifier(random_state=1, max_iter=10000).fit(X_train, y_train)\n",
    "    # caso queiramos ver os resultados, dar return ao predict\n",
    "    clf.predict(X_test)\n",
    "    return clf.score(X_test, y_test)\n",
    "\n",
    "def knn(df, X, target):\n",
    "    knn = sk.neighbors.KNeighborsClassifier(n_neighbors=7)\n",
    "    y = df[target]\n",
    "    X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(X, y, stratify=y, random_state=1)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    #Predict the response for test dataset\n",
    "    \n",
    "    y_pred= knn.predict(X_test)\n",
    "    print(y_pred)\n",
    "    print(\"Accuracy:\",sk.metrics.accuracy_score(y_test, y_pred))\n",
    "    \n",
    "def kMeans(df, X, target, header):\n",
    "    y = df[target]\n",
    "    X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(X, y, stratify=y, random_state=1)\n",
    "    clf = sk.cluster.KMeans(n_clusters=3, random_state=1).fit(X_train, y_train)\n",
    "    # caso queiramos ver os resultados, dar return ao predict\n",
    "    y_pred = clf.predict(X_test)\n",
    "    #print(sk.metrics.silhouette_samples(X_test, y_pred))\n",
    "    print(sk.metrics.silhouette_score(X_test, y_pred))\n",
    "    return clf.score(X_test)\n",
    "\n",
    "def dbscan(df, X, target):\n",
    "    y = df[target]\n",
    "    neigh = sk.neighbors.NearestNeighbors(n_neighbors=2)\n",
    "    nbrs = neigh.fit(X)\n",
    "    distances, indices = nbrs.kneighbors(X)\n",
    "    distances = np.sort(distances, axis=0)\n",
    "    distances = distances[:,1]\n",
    "    plt.plot(distances)\n",
    "    \n",
    "    clf = sk.cluster.DBSCAN(eps=3, min_samples=3).fit_predict(X, y)\n",
    "    # caso queiramos ver os resultados, dar return ao predict\n",
    "    #clf.predict(X_test)\n",
    "    return clf[:500]\n",
    "\n",
    "def agglomerativeClustering(df, X, target):\n",
    "    y = df[target]\n",
    "    clf = sk.cluster.AgglomerativeClustering(n_clusters = 2).fit_predict(X, y)\n",
    "    # caso queiramos ver os resultados, dar return ao predict\n",
    "    #clf.predict(X_test)\n",
    "    return clf[:500]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c819e0c0-6091-49d7-bd26-49477e9321be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1599 0.1 1\n",
      "0.84\n",
      "0.8425\n"
     ]
    }
   ],
   "source": [
    "dataset, header = loadFile(\"./winequality-red.csv\")\n",
    "header[11] = header[11].strip('\\n')\n",
    "redMissing = removeValues(dataset, 0.1)\n",
    "redMissing = handleMissingValues(redMissing, 'mean')\n",
    "#redMissing = handleMissingValues(redMissing, 'most_frequent')\n",
    "\n",
    "'''\n",
    "red1 = applyNormalization(dataset, 'l1')\n",
    "print(red1)\n",
    "red2 = applyNormalization(dataset, 'l2')\n",
    "print(red2)\n",
    "red3 = applyNormalization(dataset, 'max')\n",
    "print(red3)\n",
    "\n",
    "redRed = applyDataReduction(dataset)\n",
    "\n",
    "redDisc = applyDiscretization(dataset)\n",
    "'''\n",
    "df = pd.DataFrame(dataset, columns = header)\n",
    "df[\"wine classification\"] =  [\"good\" if i >= 7 else (\"bad\" if i < 5 else \"medium\") for i in df['quality']]\n",
    "\n",
    "X = df.drop(\"wine classification\", axis=1)\n",
    "X = X.drop(\"quality\", axis=1)\n",
    "\n",
    "df = pd.DataFrame(redMissing, columns = header)\n",
    "df[\"wine classification\"] =  [\"good\" if i >= 7 else (\"bad\" if i < 5 else \"medium\") for i in df['quality']]\n",
    "\n",
    "X1 = df.drop(\"wine classification\", axis=1)\n",
    "X1 = X1.drop(\"quality\", axis=1)\n",
    "\n",
    "#plotAllPearson(dataset, header)\n",
    "\n",
    "#decisionTree(df, X, \"wine classification\")\n",
    "\n",
    "#ourMLPClassifier(df, X, \"wine classification\")\n",
    "\n",
    "#knn(df, X, \"wine classification\")\n",
    "\n",
    "#kMeans(df, X, \"wine classification\", header)\n",
    "\n",
    "#dbscan(df, X, \"wine classification\")\n",
    "\n",
    "#agglomerativeClustering(df, X, \"wine classification\")\n",
    "\n",
    "print(ourMLPClassifier(df, X, \"wine classification\"))\n",
    "\n",
    "print(ourMLPClassifier(df, X1, \"wine classification\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9789051b-b6c7-485a-87bf-c9761bfd1f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7938a4-3709-4655-ac59-9b9d3f9a2c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
